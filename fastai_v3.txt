fastai course v3 notes

# found useful
free -m
docker exec -it container_id bash
jupyter notebook list
# sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/data:/data -v /home/xbno/ml/course-v3:/course-v3 --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/course-v3/data:/data -v /home/xbno/course-v3/code/course-v3-me:/notebooks/course-v3-me -v /home/xbno/course-v3/pretrained/.torch:/root/.torch --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
find / -name "stage-1-50*"
docker container stop $(docker container ls -aq)
docker container rm $(docker container ls -aq)

# functions
doc()
    has links to documentation
help()
    shows info

ImageDataBunch()
    main data object that pulls in data from directory etc
.from_df()
.from_csv()
.from_folder()
    most common way to load images, imagenet style
    /train/categories and /test/categories

learn()
    main object in fastai that encompases both data and a model
.load()
    load saved model, dunno where
.lr_find()
    saves the recorder below based on mock loss of the data and lr
.recorder.plot()
    display lr curve

#-######################-#
#       lesson 1
#-######################-#

high loss predictions
    ones that our model is most confident in but got wrong

training progression
    start with high lr (~.001 aka 1e-3) or (3e-3 is a jeremys fav) fine tuning last layers
    follow up with a full tune of all layers with a lr_max of something between 1e-6
    and soemthign 10x smaller than what you started with or clearly not close to the
    edge of lr cliff

    you know you have a good training rate if your error doesn't explode and it
    isn't sluggishly slow either.

    time to get more data when you select a proper learning rate, and your loss goes
    down and then begins going up, and you're still unhappy with your accuracy

validation loss:
    if its enormous, the learning rate is too high

train loss:
    always aim to train loss lower than validation loss. if its higher your underfitting.
    bump up # of epochs or learning rate.
    train the last bit at a lower learning rate?
    or reduce regularization by adding weight ecay, data aug or
    ***this isn't stated by all in deep learning, so may read otherwise***

overfitting in DL:
    your error rate improves for a while and then starts getting worse

oversampling shouldn't be required unless you find your lower samples

if run out of mem, change the bs (batchsize) in the databunch object

recommended to seed prior to running the image data bunch so it picks a repeatable set
    of images because then you can concretely confirm if hyperparam tuning actually helps
    the model since it will be run on the same images again: np.random.seed(42)

if you find activation maps within a convnet that produces zeros for a lot of different
    inputs, you probably have deadfilters. this is a symptom of high learning rates

#-######################-#
#       lesson 2
#-######################-#

tensor
    rank 1 tensor: torch.tensor(np.array([1,2,3]))
    rank 3 tensor: image rows x columns x channels (r,g,b)
    rank 4 tensor: num_images x rows x columns x channels (r,g,b)

gradient decent:
    calculates the loss on the whole data set (x) each time it updates params
    ex:
        y_hat = x@a
        loss = mse(y, y_hat)

stocastic gradient decent:
    calculates the loss on a sample of the dataset (x[rand_idx]) and updates params
    will bounce around even without a high learning rate
    ex:
        rand_idx = np.random.choice([i for i in range(n)],s,replace=False)
        y_hat = x[rand_idx]@a
        loss = mse(y[rand_idx], y_hat)


sgd vs gd:
    the max learning rate appropriate will change based on the batch size etc.
    meaning choosing whats in your batches is REALLY important (maybe this is already done?)
    which is why lightfm's warp was sneaky good, because it chose only to do updates on samples
        that were very wrong. look into this

learning rate (lr):
    is what we mulitply our gradient by to update our params

epoch:
    one complete run through all our data. when doing batches, this means all batches
        seen have included all data
    generally want to keep epochs down as much as possible, since the more epochs we see
        the more times we've seen the same data which gets makes it more likely to overfit

architecture:
    the mathmatical model you're fitting the parameters to
    y = x1*a1+x2*a2 (a is the params), which is m*x + b
    y = resnet34(image)

parameters (weights or coeficients):
    are the numbers that you're updating

#-######################-#
#       lesson 3
#-######################-#

pytorch dataset/fastai databunch logical structure is:
    torch.utils.DataSet() is used by torch.utils.data.DataLoader() is used by fastai.
    DataLoader:
        combines dataset into a minibatch ready for model consumption
    DataBunch:
        composed of two DataLoaders for train and validation sets

fastai DataBlock api:
    creates everything fort he databunch object

tfms get_transforms():
    filp_vert = True

last layer choices:
    sigmoid:
        makes each neuron in the last layer independent of one another. therefore
        one could have a score of .992 and another with a score of .995
    softmax:
        normalizes all neuron values in the last layer to add up to 1, making
        the output an interpretable probability

accuracy:
    single-class:
        with a standard single class problem there will be n outputs of the model
        each will correspond to the confidence (because the last layer is a sigmoid)
        of the class. therefore you use an argmax to find the most likely class based
        on the class with the highest confidence
        func: accuracy
    multi-class:
        with a multi class problem there will be numerous outputs each that an image
        or input could be labeled with. therefore you need to select a threshold
        an output neuron needs to exceed to be labeled.
        func: accuracy_threshold(thresh=.2)

partial:
    **since we always want to call this function with a thesh=.2 you can use
    a python 3 thing called acc_02 = partial(func,keyword=value) to call it.**

    def average(l,num):
        return np.random.choice(l,num,replace=False).mean()

    def a3(l):
        return average(l,3)

    a3 = partial(average,num=3)

    average([1,2,3],3) == a3(l)

for misclassified data:
    recommends taking the saved original model, using a new databunch with the
    misclassified data, and finetuning (unfreezing?) model. test and play most likely
    but perhaps using a higher lr

lr_find:
    we want to hit the steepest negative slope so that we can slide down to the bottom.
    typically before you unfreeze sliding from higher 1e-4ish to the bottom ~1e-2ish

progressive resizing:
    the idea of training the model on 64x64 then 128x128 then 256x256.
    a form of transfer learning?
    how does the same model take a different input so easily?
    **seems as though it is less sensitive to frozen training as you progressively
    train on larger images. perhaps lower number of fits or unfreeze quicker

fit_one_cycle:
    actually bumps the lr up at the beginning and then brings it down

    uses learning rate annealing (gradually decreasing the lr while fitting)

plot_losses:
    if you find the loss increases a bit and then decreases you've found a good lr

mixed precision training:
    16bits instead of 32bits precision
    learner.to_fp16()

normalizing:
    when training with a pretrained model, you don't want to normalize your data
    prior to inputting since you'll normolize the data to itself and lose the
    learning the pretrained model has built. instead

#-######################-#
#       lesson 4
#-######################-#

language model:
    a model that learns to predict the next word of the sequence. at this point
    i think of this type of training as learning the fundamentals of words/sentace
    structure. kinda like how a pretrained resnet34 has learned the low level
    edge detectors, gradient shadows, etc in the first conv layers.

typical approach:
    1. take a full wiki language model
    2. finetune the model to the specific language corpus (imdb reviews)
    3. use the language model to create a classifier which will output the answers.
        requires the same vocab as the finetuned lnaguage model (#2)

typical acc with a lanuague model:
    for specific domain like legal or medical acc can be >= .5
    for imdb reviews acc ~.3 is pretty good

numericalization:
    basically tokenize words
    do

language models (rnns in general?):
    encoder - the bit that learns how to undertsand the sentance so far (context?)
    generater - the bit that learns how to predict the next word

tabular data in deeplearning:
    lessens the requirement of feature engineering

#-######################-#
#       lesson 5
#-######################-#

parameters:
    the weight tensors of the nueral net
    these will be random when new
    these are pre-set (pre-learned) when taking a previous trained net and finetuning it
        the lower layer weights are more fundamental
        the layers closer to the output are more specific to the pretrained net goal

activations:
    inputs, outputs
    the calculated values that arise from either matrix multiplications or nonlinearities

activation functions:
    element-wise functions that add non-linearity to nns
    they don't matter so much, so relu is standard

    relu - zero negatives, multiply by 1 for larger than 1
    sigmoid -
    cross entropy

back propogation:
    calculate the gradients for all weights
    weights - gradients * learning rate

discriminative learning rates:
    mainly used with transfer learning
    basically splitting the architecture of a model then assigning
        lower learning rates to the earlier layers
        higher learning rates to the later layers

learning rates in fastai (divided by 3 because of something to do with batchnorm)
    .001 (1e-3)
        all layers with the same learning rate
    slice(1e-3)
        last layer group at lr=1e-3
        rest at lr=(1e-3/3)
    slice(1e-5,1e-3) will train the
        last layer group at lr=1e-3
        middle layer groups evenly split between 1e-3 and 1e-5
        first layer group at lr=1e-5

layer groups
    for pretrained architetures the new layers are 1 group
    the rest of the model is split into 2 groups by default

afine function:
    not always matrix multiplication between layers
    but they may be linear functions (like cnn layers where weigths are tied together)
    both are afine functions

embeddings - array lookup vs one-hot encoding
    one-hot encoded matrix times an embedding matrix outputs the correct user embeddings
    array lookup does the same thing but uses much less memory because since it doesn't have to mulitply anyting
    always use the array lookup version which is really just embedding

bias
    add value of global likeness or badness to movies etc
    instead of adding a frozen weight of 1 to the embedding libraries specifically add bias
    can reduce overall error even further

latent factors:
    since the embeddings

epoch:
    means your network is looking at every input once
    10 epochs means your looking at all you input data 10x times

remember:
    if you put an afline function with another afine function you get nothing
    you always need a non-linearity between layers or else you can't learn anything

sigmoid collab range:
    since sigmoids assomtote at whateveer value you set, they have a very hard time ever reaching it
    so set the sigmoid function to be a little lower a little higher than the values

embeddings:
    recommended to reduce the embeddings down to a resonable size before looking for similars
    very often a good idea to chuck weights from nueral nets then put them thru pca then analyze
    are just really cool in terms of learning interesting similarities between things
    good idea is to train them in one space then use them elsewhere like standard RF etc

weight-decay:
    type of regularization
    sounds exactly like l2 reg where lambda is just wd
    typically set to .01 but .1 works too, might just not ever overfit and therefore never quite get perfect

overfitting:
    standard idea of less complex is less parameters via stats
    but really counting the number of parameters to determine complexity is all wrong
    more params = more non-linearities which is good
    penalize complexity is regularization
        summing upp all params doesn't wwork because some are negative
        thats why l1 is sum of abs of params
        and why l2 is params**2
    thing is that what if your best loss is actually setting all the params to 0 since theyre being **2

data bunch:
    a convinience wrapper to take a full dataset of x,y and create tuples of them

minibatch:
    a set of the full data chunked into a batch size of 64 or whatever

online gradient decent:
    every sample is used in gradient decent

gradient decent:
    how far your prediction was off from your actual / how far your change was

momentum:
    .9 is most common
    changes how the gradient is updated where it uses .1 * the derivative of this time and .9 * the last value
    basically an exponentially weighted moving average of the last few values

rmsprop:


adam:
    momentum + rmsprop
    keeps track of the exponential moving average of the steps
    and keeps track of the exponential moving average of the gradient

learning rate per batch:
    start small because the weights are random so we want to move slowly
        and if you jump around because of the big gradients related to the random weights they'll throw you off
    then when you get to sensable weights you want to have a higher learning rate where you end up moving in the right direction
    then as you get close the final answer you want to start slowing down to hone in
    when you have this low learning rate but you keep going in the right direction you might as well go faster aka high momentum
    and then if your jumping really far with your learning rate, don't compound it with high momentum, because it might throw you off
    one_cycle does this where it can let you do super convergence

fastai losses
    doesn't show you actual loss
    rather shows the exponentially weighted moving average of the loss so its easier to visualize

cross entropy loss:
    penalizes very confident wrong predictions highly
    requires activations coming into the loss function to add up to 1
        because they're all probabilities
    meaning you have to use a softmax non-linearity of your last layer

softmax:
    does e ** activations (becasue that'll make them all positive)
    then sums those numbers and divides them all by it
    sinlge lable multiclass classification

pytorch actually does softmax cross entropy loss
this means you might find your classifier outputs some crazy negs and pos numbers, it probably just needs a softmax at the end of it to use it

regularization in nueral nets:
    weight decay
    batchnorm
    dropout
    data augmentation

#-######################-#
#       lesson 6
#-######################-#



#-######################-#
#       Bert
#-######################-#

link to bert: https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html

context-free vs contextual:
    word2vec and glove are context free - meaning they create the same embedding for the word
        bank whether it is used a 'bank fo a river' or 'bank heist'
    bert (and some other pre-trained representations) are contextual - meaning they generate
        a representaion for each word based on the other words in the sentence. while a uni-directional
        model would only see the previous words of the context, a bidirectional model (like bert)
        uses both words prior to and after the word of interest to generate its representation.

zero-shot:
    zero-shot learning is

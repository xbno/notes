fastai course v3 notes

#-######################-#
#       found useful in setup
#-######################-#

free -m
docker exec -it container_id bash
jupyter notebook list
# sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/data:/data -v /home/xbno/ml/course-v3:/course-v3 --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/course-v3/data:/data -v /home/xbno/course-v3/code/course-v3-me:/notebooks/course-v3-me -v /home/xbno/course-v3/pretrained/.torch:/root/.torch --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
find / -name "stage-1-50*"
docker container stop $(docker container ls -aq)
docker container rm $(docker container ls -aq)
service docker restart

# on amazon gpu
sudo service docker stop
sudo nano /etc/docker/daemon.json
{
    "data-root": "/pnt/docker",
}
sudo service docker start
sudo docker run --runtime=nvidia -d -p 8888:8888 -v /pnt:/data --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6

# locally
conda install seaborn, graphviz
conda install opencv
conda install -y bcolz


# functions
doc()
    has links to documentation
help()
    shows info

ImageDataBunch()
    main data object that pulls in data from directory etc
.from_df()
.from_csv()
.from_folder()
    most common way to load images, imagenet style
    /train/categories and /test/categories

learn()
    main object in fastai that encompases both data and a model
.load()
    load saved model, dunno where
.lr_find()
    saves the recorder below based on mock loss of the data and lr
.recorder.plot()
    display lr curve

#-######################-#
#       lesson 1
#-######################-#

fine grained classification:
classification between categories that are very similar like facial recog or between breeds
    when your aim is to classify differences between things of the same class
    http://www.cs.umd.edu/~djacobs/CMSC733/FineGrainedClassification.pdf

size of images:
    ability of gpus to calculate fast is to do the same thing to a bunch of same sized items at once
    any input whether image or text or else needs to be same size input tensor for a given model
    default to size 224, typical setting

pixels:
    0-255 range
    require normalize

transfer learning:
    the focus of the course and jeremys work recently
    take a model that already knows how to do something pretty well and then make it do your problem very well
    ie. use resnet34 which has been pretrained to classify 1000 categories in imagenet
        meaning that it already knows what animals, and pets, and images look like already
        then fine tune it to classify the types of breeds of cats or dogs
    by doing this you can train models in 1/100th the time it would take with 1/100th or less data than in regular (from scratch training)

 
high loss predictions
    ones that our model is most confident in but got wrong
    a subset of examples that we can use to intrepret how the model is performing
        and give us ideas of how to improve the model

training progression
    start with high lr (~.001 aka 1e-3) or (3e-3 is a jeremys fav) fine tuning last layers
    follow up with a full tune of all layers with a lr_max of something between 1e-6
    and soemthign 10x smaller than what you started with or clearly not close to the
    edge of lr cliff

    you know you have a good training rate if your error doesn't explode and it
    isn't sluggishly slow either.

    time to get more data when you select a proper learning rate, and your loss goes
    down and then begins going up, and you're still unhappy with your accuracy

validation loss:
    if its enormous, the learning rate is too high

train loss:
    always aim to train loss lower than validation loss. if its higher your underfitting.
    bump up # of epochs or learning rate.
    train the last bit at a lower learning rate?
    or reduce regularization by adding weight ecay, data aug or
    ***this isn't stated by all in deep learning, so may read otherwise***

good resource for loss:
    https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model

overfitting in DL:
    your error rate improves for a while and then starts getting worse

oversampling shouldn't be required unless you find your lower samples

if run out of mem, change the bs (batchsize) in the databunch object

recommended to seed prior to running the image data bunch so it picks a repeatable set
    of images because then you can concretely confirm if hyperparam tuning actually helps
    the model since it will be run on the same images again: np.random.seed(42)

if you find activation maps within a convnet that produces zeros for a lot of different
    inputs, you probably have deadfilters. this is a symptom of high learning rates

#-######################-#
#       lesson 2
#-######################-#

tensor
    rank 1 tensor: torch.tensor(np.array([1,2,3]))
    rank 3 tensor: image rows x columns x channels (r,g,b)
    rank 4 tensor: num_images x rows x columns x channels (r,g,b)

gradient decent:
    calculates the loss on the whole data set (x) each time it updates params
    ex:
        y_hat = x@a
        loss = mse(y, y_hat)

stocastic gradient decent:
    calculates the loss on a sample of the dataset (x[rand_idx]) and updates params
    will bounce around even without a high learning rate
    ex:
        rand_idx = np.random.choice([i for i in range(n)],s,replace=False)
        y_hat = x[rand_idx]@a
        loss = mse(y[rand_idx], y_hat)

minibatch sgd vs sgd vs gd:
    minibatch sgd and updates params every minibatch, goldilocks version decent amount of updates decent amount confidence in them
    sgd updates params each and every sample given, lots of updates fuctuating all the time
    gd updates params after all samples are learned from leading to 'globally good' updates but very few updates

batch size:
    the max learning rate appropriate will change based on the batch size (larger batchsize should mean larger maintainable lr)
    meaning choosing whats in your batches is REALLY important (maybe this is already done?)
    which is why lightfm's warp was sneaky good, because it chose only to do updates on samples
        that were very wrong. look into this

learning rate (lr):
    is what we mulitply our gradient by to update our params

epoch:
    one complete run through all our data. when doing batches, this means all batches
        seen have included all data
    generally want to keep epochs down as much as possible, since the more epochs we see
        the more times we've seen the same data which gets makes it more likely to overfit

architecture:
    the mathmatical model you're fitting the parameters to
    y = x1*a1+x2*a2 (a is the params), which is m*x + b
    y = resnet34(image)

parameters (weights or coeficients):
    are the numbers that you're updating

#-######################-#
#       lesson 3
#-######################-#

pytorch dataset/fastai databunch logical structure is:
    torch.utils.DataSet() is used by torch.utils.data.DataLoader() is used by fastai.
    DataLoader:
        combines dataset into a minibatch ready for model consumption
    DataBunch:
        composed of two DataLoaders for train and validation sets

train/valid/test:
    train has labels and is used to train your model (usually like 80-90% of labeled data)
    valid has labels and is used to 'test' your model (usually like 10-20% of labeled data)
    test doesn't have labels and is used for inference or to generate answers for kaggle or to somebody who has held out the labels to judge algos

fastai DataBlock api:
    creates everything fort he databunch object

tfms get_transforms():
    filp_vert = True

last layer choices:
    sigmoid:
        makes each neuron in the last layer independent of one another. therefore
        one could have a score of .992 and another with a score of .995
    softmax:
        normalizes all neuron values in the last layer to add up to 1, making
        the output an interpretable probability

accuracy:
    single-class:
        with a standard single class problem there will be n outputs of the model
        each will correspond to the confidence (because the last layer is a sigmoid)
        of the class. therefore you use an argmax to find the most likely class based
        on the class with the highest confidence
        func: accuracy
    multi-class:
        with a multi class problem there will be numerous outputs each that an image
        or input could be labeled with. therefore you need to select a threshold
        an output neuron needs to exceed to be labeled.
        func: accuracy_threshold(thresh=.2)

partial:
    **since we always want to call this function with a thesh=.2 you can use
    a python 3 thing called acc_02 = partial(func,keyword=value) to call it.**

    def average(l,num):
        return np.random.choice(l,num,replace=False).mean()

    def a3(l):
        return average(l,3)

    a3 = partial(average,num=3)

    average([1,2,3],3) == a3(l)

for misclassified data:
    recommends taking the saved original model, using a new databunch with the
    misclassified data, and finetuning (unfreezing?) model. test and play most likely
    but perhaps using a higher lr

lr_find:
    we want to hit the steepest negative slope so that we can slide down to the bottom.
    typically before you unfreeze sliding from higher 1e-4ish to the bottom ~1e-2ish

progressive resizing:
    the idea of training the model on 64x64 then 128x128 then 256x256.
    a form of transfer learning?
    how does the same model take a different input so easily?
    **seems as though it is less sensitive to frozen training as you progressively
    train on larger images. perhaps lower number of fits or unfreeze quicker

fit_one_cycle:
    actually bumps the lr up at the beginning and then brings it down

    uses learning rate annealing (gradually decreasing the lr while fitting)

plot_losses:
    if you find the loss increases a bit and then decreases you've found a good lr

mixed precision training:
    16bits instead of 32bits precision
    learner.to_fp16()

normalizing:
    when training with a pretrained model, you don't want to normalize your data
    prior to inputting since you'll normolize the data to itself and lose the
    learning the pretrained model has built. instead

#-######################-#
#       lesson 4
#-######################-#

language model:
    a model that learns to predict the next word of the sequence. at this point
    i think of this type of training as learning the fundamentals of words/sentace
    structure. kinda like how a pretrained resnet34 has learned the low level
    edge detectors, gradient shadows, etc in the first conv layers.

typical approach:
    1. take a full wiki language model
    2. finetune the model to the specific language corpus (imdb reviews)
    3. use the language model to create a classifier which will output the answers.
        requires the same vocab as the finetuned lnaguage model (#2)

typical acc with a lanuague model:
    for specific domain like legal or medical acc can be >= .5
    for imdb reviews acc ~.3 is pretty good

numericalization:
    basically tokenize words
    do

language models (rnns in general?):
    encoder - the bit that learns how to undertsand the sentance so far (context?)
    generater - the bit that learns how to predict the next word

tabular data in deeplearning
    lessens the requirement of hand creating features
    pinterest changed over their models from xgboost to deeplearning
    all thats really required is to define categorical and continuous features
        categoricals will be replaced with embeddings
        continuous can be input directly

#-######################-#
#       lesson 5
#-######################-#

parameters:
    the weight tensors of the nueral net
    these will be random when new
    these are pre-set (pre-learned) when taking a previous trained net and finetuning it
        the lower layer weights are more fundamental
        the layers closer to the output are more specific to the pretrained net goal

activations:
    inputs, outputs
    the calculated values that arise from either matrix multiplications or nonlinearities

activation functions:
    element-wise functions that add non-linearity to nns
    they don't matter so much, so relu is standard

    relu - zero negatives, multiply by 1 for larger than 1
    sigmoid -
    cross entropy

back propogation:
    calculate the gradients for all weights
    weights - gradients * learning rate

discriminative learning rates:
    mainly used with transfer learning
    basically splitting the architecture of a model then assigning
        lower learning rates to the earlier layers
        higher learning rates to the later layers

learning rates in fastai (divided by 3 because of something to do with batchnorm)
    .001 (1e-3)
        all layers with the same learning rate
    slice(1e-3)
        last layer group at lr=1e-3
        rest at lr=(1e-3/3)
    slice(1e-5,1e-3) will train the
        last layer group at lr=1e-3
        middle layer groups evenly split between 1e-3 and 1e-5
        first layer group at lr=1e-5

layer groups
    for pretrained architetures the new layers are 1 group
    the rest of the model is split into 2 groups by default

afine function:
    not always matrix multiplication between layers
    but they may be linear functions (like cnn layers where weigths are tied together)
    both are afine functions

embeddings - array lookup vs one-hot encoding
    one-hot encoded matrix times an embedding matrix outputs the correct user embeddings
    array lookup does the same thing but uses much less memory because since it doesn't have to mulitply anyting
    always use the array lookup version which is really just embedding

bias
    add value of global likeness or badness to movies etc
    instead of adding a frozen weight of 1 to the embedding libraries specifically add bias
    can reduce overall error even further

latent factors:
    since the embeddings

epoch:
    means your network is looking at every input once
    10 epochs means your looking at all you input data 10x times

remember:
    if you put an afline function with another afine function you get nothing
    you always need a non-linearity between layers or else you can't learn anything

sigmoid collab range:
    since sigmoids assomtote at whateveer value you set, they have a very hard time ever reaching it
    so set the sigmoid function to be a little lower a little higher than the values

embeddings:
    recommended to reduce the embeddings down to a resonable size before looking for similars
    very often a good idea to chuck weights from nueral nets then put them thru pca then analyze
    are just really cool in terms of learning interesting similarities between things
    good idea is to train them in one space then use them elsewhere like standard RF etc

weight-decay:
    type of regularization
    sounds exactly like l2 reg where lambda is just wd
    typically set to .01 but .1 works too, might just not ever overfit and therefore never quite get perfect

overfitting:
    standard idea of less complex is less parameters via stats
    but really counting the number of parameters to determine complexity is all wrong
    more params = more non-linearities which is good
    penalize complexity is regularization
        summing upp all params doesn't wwork because some are negative
        thats why l1 is sum of abs of params
        and why l2 is params**2
    thing is that what if your best loss is actually setting all the params to 0 since theyre being **2

data bunch:
    a convinience wrapper to take a full dataset of x,y and create tuples of them

minibatch:
    a set of the full data chunked into a batch size of 64 or whatever

online gradient decent:
    every sample is used in gradient decent

gradient decent:
    how far your prediction was off from your actual / how far your change was

momentum:
    .9 is most common
    changes how the gradient is updated where it uses .1 * the derivative of this time and .9 * the last value
    basically an exponentially weighted moving average of the last few values

rmsprop:


adam:
    momentum + rmsprop
    keeps track of the exponential moving average of the steps
    and keeps track of the exponential moving average of the gradient

learning rate per batch:
    start small because the weights are random so we want to move slowly
        and if you jump around because of the big gradients related to the random weights they'll throw you off
    then when you get to sensable weights you want to have a higher learning rate where you end up moving in the right direction
    then as you get close the final answer you want to start slowing down to hone in
    when you have this low learning rate but you keep going in the right direction you might as well go faster aka high momentum
    and then if your jumping really far with your learning rate, don't compound it with high momentum, because it might throw you off
    one_cycle does this where it can let you do super convergence

fastai losses
    doesn't show you actual loss
    rather shows the exponentially weighted moving average of the loss so its easier to visualize

cross entropy loss:
    penalizes very confident wrong predictions highly
    requires activations coming into the loss function to add up to 1
        because they're all probabilities
    meaning you have to use a softmax non-linearity of your last layer

softmax:
    does e ** activations (becasue that'll make them all positive)
    then sums those numbers and divides them all by it
    sinlge lable multiclass classification

pytorch actually does softmax cross entropy loss
this means you might find your classifier outputs some crazy negs and pos numbers, it probably just needs a softmax at the end of it to use it

regularization in nueral nets:
    weight decay
    batchnorm
    dropout
    data augmentation

#-######################-#
#       lesson 6
#-######################-#

tabluar data:
    break everything into datetime parts

dropout:
    for each minibatch throw away a percentage of the activations
    common value of p (drop perc) is .5
    typically only in the hidden layers, not the input or output layers
    goal to make no one activation or set of activations memorize an input to output ultimately regularizing
    needs to be activated during trianing but NOT during inference aka prediction
    instead of doing this pytorch and other networks actually multiply all weights during training time by 1/p
    makes it so that you don't actually need to do anything during inference

batchnorm:
    bit of regularization
    bit of trining helper
    aids in reducing bumpyness of the loss vs epoch relationship
    ultimately lets you use higher learning rates
    works in minibatches
    its a layer that takes in activations and adds both multiplicative bias and added bias
    ultimately makes it easier to scale layers influence up or down by these two learnable parameters
    also its key that it uses momentum when determining the mean and variance it uses to calc bn
        takes an exponentially wieghted average of the mean and variance so they don't bounce around so much between mini batches
        but the mean and variance in jeremys words aren't the most important part of bn. so perhaps you could get away with just the biases
    smaller momentum leads to less bouncing of the mean and variance so it has less of a regularization affect

data augmentation:
    jeremy is most excited about it as a regularization method - mainly because there haven't eben definative studies on the matter
    nobody is doing data aug in text data or tabular or medical genomics anything like that
    decreases data req by like 5-10x because its basically free extra labeled data

convs:

average pool:

hook:

#-######################-#
#       lesson 7
#-######################-#

data blocks api:
    1. create an item list (il)
        list of items that it will create data from
        case of images its a list of images
        case of df its a list of samples
        il.items[0] gets you the first sample
    2. split data into train vs validation (sd)
        .split_by_folder .no_split for no validation
        cannot skip entirely
    3. label it and create a label list (ll)
        create the labels via methos like
        label_from_folder label_from_re
    4. setup transforms
        set transforms for both training and validation
        .transforms(([train transforms],[validation transforms]))
        these are applied when grabbing the data from disk (at least for images)
    5. create a data bunch
        set your batch size
        normalize data if your not using a pretrained model
        data.train_ds is a data set
        data.train_ds[0] will give an x and a y sample
        data.one_batch() will give a transformed data batch
        data.show_batch() will show samples and their labels

deep residual networks and learning (paper):
    they noticed that the deep (large param space) network performed worse than the shallonw one and said hmm
    figured he could skip all the conv layers at the end by putting a skip connection in and that the network would
        shouldn't perform any worse than the original smaller network
    skip connection is one where the input is fed around the layers so the network in theory the model can use
        set the weights to zero in those layers and bypass them altogether
        by adding them to the outputs (+)
    skip connections or res-blocks are revolutionary and pretty much can always outperform networks without (?)

fastai conv_layer:
    conv batchnorm relu chunks

visualizing the loss landscape (paper):
    they plotted loss over a x,y (two dimensions probably pca'ed down)
    then showed that res-blocks drastically smooth the loss landscape proving why res-blocks really work

resnets:
    genuinely really simple and easy to implement

densenets:
    same concept as res-blocks except instead of adding them to the output of a layer they're concatted
    concats each input to the next output essentially keeping the original input features the entire way
        through the layers
    very memory intensive but typically few params
    they work well with segmentation because they keep the input
    in practice theres something else going on that downsamples the dense concatted input since its a diff size than the input to the next layer

refactoring:
    do if often and consistenly, you'll make less mistakes when things are bundled

unet:
    imaging segmentation model that uses skip connections (concatting) across the down and upsampling parts
        pre res-net which is pretty clever
    downsampling part reduces pic down to is conv interpretation to somehting really dense like 512 x 7 x 7
    upsampling part increases each channel by conv'ing it against a larger version of the channel
        padding with pad pixels in between
        nearest neighbor interpolation

note:
    generally you want as much interactions between added or concatted inputs with the outputs of that channel as possible

gan to generate higher quality pics (decrappify):
    basic idea is to start with high qual pics, transform them into crappy versions of the same thing
    then train a model on the crappy input pic with the high quality output image as the 'truth' image

generative model:
    output is an image that created as opposed to a number of class

think about how loss function relates to your goal:
    *needs to mathmatically descibe what we want
    mse of pred to truth on pixel level gets the model to take care of really large issues in input image
        but doesn't solve the low to high res problem since the mse between pixels isn't a great metric
    before watching the rest im thinking that a loss function that includes the error of the min and max of small regions of the photo would be in the right direction

gan:
    actually does this by using a loss function that calls another model
    so the model you call is actually trying to classify the prediction and the truth as whether theyre the truth or the generated image
    so by using a critic t as the loss, and if we can fool it into thinking the generated image is teh truth, then its gotta be pretty good at generating images
    the critic is really just a binary cross entropy loss function
    you need to bounce between training the generator to create images and training the critic into being a good critiquer
    to make it work you need to stack losses of both the critic and the mse of pixels
        because otherwise you could be generating really good images that have no similarity to the original picture
    since each of those two losses are on different scales you need to get your pixel loss to be on the same order as the critic
        usually multiply by 50 to 200
    also since your updating the critic to get better and better as you go its hard to determine how well its working because the loss will be ablout the same over time
        so the best way to determine how well you're doing is by taking a look at the batches over the course of training

unets:
    use when the resolution of the input and output is roughly the same

reclaiming gpu mem:
    set something you know is using a lot of mem to = None
    then do gc.collect() to garbage collect

feature losses:
    grab some middle activations of the generator model and compare the pred and thruth activations of that layer and do mse on it
    using the idea that the features are just as important as the actual loss
    basically its trying to judge a prediction on the featuers which will represent the pic

generally when joining layers they're either added or concatted

jeremys advice:
    do something. 100% through
    when you get to that 80% finished point, add a readme make sure you have a proper install script
    finish things completely.

#-######################-#
#       Bert
#-######################-#

link to bert: https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html

context-free vs contextual:
    word2vec and glove are context free - meaning they create the same embedding for the word
        bank whether it is used a 'bank fo a river' or 'bank heist'
    bert (and some other pre-trained representations) are contextual - meaning they generate
        a representaion for each word based on the other words in the sentence. while a uni-directional
        model would only see the previous words of the context, a bidirectional model (like bert)
        uses both words prior to and after the word of interest to generate its representation.

zero-shot:
    zero-shot learning is

fastai course v3 notes

#-######################-#
#       found useful in setup
#-######################-#

free -m
docker exec -it container_id bash
jupyter notebook list
# sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/data:/data -v /home/xbno/ml/course-v3:/course-v3 --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/course-v3/data:/data -v /home/xbno/course-v3/code/course-v3-me:/notebooks/course-v3-me -v /home/xbno/course-v3/pretrained/.torch:/root/.torch --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
find / -name "stage-1-50*"
docker container stop $(docker container ls -aq)
docker container rm $(docker container ls -aq)
service docker restart

# on amazon gpu
sudo service docker stop
sudo nano /etc/docker/daemon.json
{
    "data-root": "/pnt/docker",
}
sudo service docker start
sudo docker run --runtime=nvidia -d -p 8888:8888 -v /pnt:/data --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6

# locally
conda install seaborn, graphviz
conda install opencv
conda install -y bcolz


# functions
doc()
    has links to documentation
help()
    shows info

ImageDataBunch()
    main data object that pulls in data from directory etc
.from_df()
.from_csv()
.from_folder()
    most common way to load images, imagenet style
    /train/categories and /test/categories

learn()
    main object in fastai that encompases both data and a model
.load()
    load saved model, dunno where
.lr_find()
    saves the recorder below based on mock loss of the data and lr
.recorder.plot()
    display lr curve

#-######################-#
#       lesson 1
#-######################-#

fine grained classification:
classification between categories that are very similar like facial recog or between breeds
    when your aim is to classify differences between things of the same class
    http://www.cs.umd.edu/~djacobs/CMSC733/FineGrainedClassification.pdf

size of images:
    ability of gpus to calculate fast is to do the same thing to a bunch of same sized items at once
    any input whether image or text or else needs to be same size input tensor for a given model
    default to size 224, typical setting

pixels:
    0-255 range
    require normalize

transfer learning:
    the focus of the course and jeremys work recently
    take a model that already knows how to do something pretty well and then make it do your problem very well
    ie. use resnet34 which has been pretrained to classify 1000 categories in imagenet
        meaning that it already knows what animals, and pets, and images look like already
        then fine tune it to classify the types of breeds of cats or dogs
    by doing this you can train models in 1/100th the time it would take with 1/100th or less data than in regular (from scratch training)

high loss predictions
    ones that our model is most confident in but got wrong
    a subset of examples that we can use to intrepret how the model is performing
        and give us ideas of how to improve the model

training progression
    start with high lr (~.001 aka 1e-3) or (3e-3 is a jeremys fav) fine tuning last layers
    follow up with a full tune of all layers with a lr_max of something between 1e-6
    and soemthign 10x smaller than what you started with or clearly not close to the
    edge of lr cliff

    you know you have a good training rate if your error doesn't explode and it
    isn't sluggishly slow either.

    time to get more data when you select a proper learning rate, and your loss goes
    down and then begins going up, and you're still unhappy with your accuracy

validation loss:
    if its enormous, the learning rate is too high

train loss:
    always aim to train loss lower than validation loss. if its higher your underfitting.
    bump up # of epochs or learning rate.
    train the last bit at a lower learning rate?
    or reduce regularization by adding weight ecay, data aug or
    ***this isn't stated by all in deep learning, so may read otherwise***

good resource for loss:
    https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model

overfitting in DL:
    your error rate improves for a while and then starts getting worse

oversampling shouldn't be required unless you find your lower samples

if run out of mem, change the bs (batchsize) in the databunch object

recommended to seed prior to running the image data bunch so it picks a repeatable set
    of images because then you can concretely confirm if hyperparam tuning actually helps
    the model since it will be run on the same images again: np.random.seed(42)

if you find activation maps within a convnet that produces zeros for a lot of different
    inputs, you probably have deadfilters. this is a symptom of high learning rates

#-######################-#
#       lesson 2
#-######################-#

tensor
    rank 1 tensor: torch.tensor(np.array([1,2,3]))
    rank 3 tensor: image rows x columns x channels (r,g,b)
    rank 4 tensor: num_images x rows x columns x channels (r,g,b)

gradient decent:
    calculates the loss on the whole data set (x) each time it updates params
    ex:
        y_hat = x@a
        loss = mse(y, y_hat)

stocastic gradient decent:
    calculates the loss on a sample of the dataset (x[rand_idx]) and updates params
    will bounce around even without a high learning rate
    ex:
        rand_idx = np.random.choice([i for i in range(n)],s,replace=False)
        y_hat = x[rand_idx]@a
        loss = mse(y[rand_idx], y_hat)

minibatch sgd vs sgd vs gd:
    minibatch sgd and updates params every minibatch, goldilocks version decent amount of updates decent amount confidence in them
    sgd updates params each and every sample given, lots of updates fuctuating all the time
    gd updates params after all samples are learned from leading to 'globally good' updates but very few updates

batch size:
    the max learning rate appropriate will change based on the batch size (larger batchsize should mean larger maintainable lr)
    meaning choosing whats in your batches is REALLY important (maybe this is already done?)
    which is why lightfm's warp was sneaky good, because it chose only to do updates on samples
        that were very wrong. look into this

learning rate (lr):
    is what we mulitply our gradient by to update our params

epoch:
    one complete run through all our data. when doing batches, this means all batches
        seen have included all data
    generally want to keep epochs down as much as possible, since the more epochs we see
        the more times we've seen the same data which gets makes it more likely to overfit

architecture:
    the mathmatical model you're fitting the parameters to
    y = x1*a1+x2*a2 (a is the params), which is m*x + b
    y = resnet34(image)

parameters (weights or coeficients):
    are the numbers that you're updating

#-######################-#
#       lesson 3
#-######################-#

pytorch dataset/fastai databunch logical structure is:
    torch.utils.DataSet() is used by torch.utils.data.DataLoader() is used by fastai.
    DataLoader:
        combines dataset into a minibatch ready for model consumption
    DataBunch:
        composed of two DataLoaders for train and validation sets

train/valid/test:
    train has labels and is used to train your model (usually like 80-90% of labeled data)
    valid has labels and is used to 'test' your model (usually like 10-20% of labeled data)
    test doesn't have labels and is used for inference or to generate answers for kaggle or to somebody who has held out the labels to judge algos

fastai DataBlock api:
    creates everything fort he databunch object

tfms get_transforms():
    filp_vert = True

last layer choices:
    sigmoid:
        makes each neuron in the last layer independent of one another. therefore
        one could have a score of .992 and another with a score of .995
    softmax:
        normalizes all neuron values in the last layer to add up to 1, making
        the output an interpretable probability

accuracy:
    single-class:
        with a standard single class problem there will be n outputs of the model
        each will correspond to the confidence (because the last layer is a sigmoid)
        of the class. therefore you use an argmax to find the most likely class based
        on the class with the highest confidence
        func: accuracy
    multi-class:
        with a multi class problem there will be numerous outputs each that an image
        or input could be labeled with. therefore you need to select a threshold
        an output neuron needs to exceed to be labeled.
        func: accuracy_threshold(thresh=.2)

partial:
    **since we always want to call this function with a thesh=.2 you can use
    a python 3 thing called acc_02 = partial(func,keyword=value) to call it.**

    def average(l,num):
        return np.random.choice(l,num,replace=False).mean()

    def a3(l):
        return average(l,3)

    a3 = partial(average,num=3)

    average([1,2,3],3) == a3(l)

for misclassified data:
    recommends taking the saved original model, using a new databunch with the
    misclassified data, and finetuning (unfreezing?) model. test and play most likely
    but perhaps using a higher lr

lr_find:
    we want to hit the steepest negative slope so that we can slide down to the bottom.
    typically before you unfreeze sliding from higher 1e-4ish to the bottom ~1e-2ish

progressive resizing:
    the idea of training the model on 64x64 then 128x128 then 256x256.
    a form of transfer learning?
    how does the same model take a different input so easily?
    **seems as though it is less sensitive to frozen training as you progressively
    train on larger images. perhaps lower number of fits or unfreeze quicker

fit_one_cycle:
    actually bumps the lr up at the beginning and then brings it down

    uses learning rate annealing (gradually decreasing the lr while fitting)

plot_losses:
    if you find the loss increases a bit and then decreases you've found a good lr

mixed precision training:
    16bits instead of 32bits precision
    learner.to_fp16()

normalizing:
    when training with a pretrained model, you don't want to normalize your data
    prior to inputting since you'll normolize the data to itself and lose the
    learning the pretrained model has built. instead

#-######################-#
#       lesson 4
#-######################-#

language model:
    a model that learns to predict the next word of the sequence. at this point
    i think of this type of training as learning the fundamentals of words/sentace
    structure. kinda like how a pretrained resnet34 has learned the low level
    edge detectors, gradient shadows, etc in the first conv layers.

typical approach:
    1. take a full wiki language model
    2. finetune the model to the specific language corpus (imdb reviews)
    3. use the language model to create a classifier which will output the answers.
        requires the same vocab as the finetuned lnaguage model (#2)

typical acc with a lanuague model:
    for specific domain like legal or medical acc can be >= .5
    for imdb reviews acc ~.3 is pretty good

numericalization:
    basically tokenize words
    do

language models (rnns in general?):
    encoder - the bit that learns how to undertsand the sentance so far (context?)
    generater - the bit that learns how to predict the next word

tabular data in deeplearning
    lessens the requirement of hand creating features
    pinterest changed over their models from xgboost to deeplearning
    all thats really required is to define categorical and continuous features
        categoricals will be replaced with embeddings
        continuous can be input directly

#-######################-#
#       lesson 5
#-######################-#

parameters:
    the weight tensors of the nueral net
    these will be random when new
    these are pre-set (pre-learned) when taking a previous trained net and finetuning it
        the lower layer weights are more fundamental
        the layers closer to the output are more specific to the pretrained net goal

activations:
    inputs, outputs
    the calculated values that arise from either matrix multiplications or nonlinearities

activation functions:
    element-wise functions that add non-linearity to nns
    they don't matter so much, so relu is standard

    relu - zero negatives, multiply by 1 for larger than 1
    sigmoid -
    cross entropy

back propogation:
    calculate the gradients for all weights
    weights - gradients * learning rate

discriminative learning rates:
    mainly used with transfer learning
    basically splitting the architecture of a model then assigning
        lower learning rates to the earlier layers
        higher learning rates to the later layers

learning rates in fastai (divided by 3 because of something to do with batchnorm)
    .001 (1e-3)
        all layers with the same learning rate
    slice(1e-3)
        last layer group at lr=1e-3
        rest at lr=(1e-3/3)
    slice(1e-5,1e-3) will train the
        last layer group at lr=1e-3
        middle layer groups evenly split between 1e-3 and 1e-5
        first layer group at lr=1e-5

layer groups
    for pretrained architetures the new layers are 1 group
    the rest of the model is split into 2 groups by default

afine function:
    not always matrix multiplication between layers
    but they may be linear functions (like cnn layers where weigths are tied together)
    both are afine functions

embeddings - array lookup vs one-hot encoding
    one-hot encoded matrix times an embedding matrix outputs the correct user embeddings
    array lookup does the same thing but uses much less memory because since it doesn't have to mulitply anyting
    always use the array lookup version which is really just embedding

bias
    add value of global likeness or badness to movies etc
    instead of adding a frozen weight of 1 to the embedding libraries specifically add bias
    can reduce overall error even further

latent factors:
    since the embeddings

epoch:
    means your network is looking at every input once
    10 epochs means your looking at all you input data 10x times

remember:
    if you put an afline function with another afine function you get nothing
    you always need a non-linearity between layers or else you can't learn anything

sigmoid collab range:
    since sigmoids assomtote at whateveer value you set, they have a very hard time ever reaching it
    so set the sigmoid function to be a little lower a little higher than the values

embeddings:
    recommended to reduce the embeddings down to a resonable size before looking for similars
    very often a good idea to chuck weights from nueral nets then put them thru pca then analyze
    are just really cool in terms of learning interesting similarities between things
    good idea is to train them in one space then use them elsewhere like standard RF etc

weight-decay:
    type of regularization
    sounds exactly like l2 reg where lambda is just wd
    typically set to .01 but .1 works too, might just not ever overfit and therefore never quite get perfect

overfitting:
    standard idea of less complex is less parameters via stats
    but really counting the number of parameters to determine complexity is all wrong
    more params = more non-linearities which is good
    penalize complexity is regularization
        summing upp all params doesn't wwork because some are negative
        thats why l1 is sum of abs of params
        and why l2 is params**2
    thing is that what if your best loss is actually setting all the params to 0 since theyre being **2

data bunch:
    a convinience wrapper to take a full dataset of x,y and create tuples of them

minibatch:
    a set of the full data chunked into a batch size of 64 or whatever

online gradient decent:
    every sample is used in gradient decent

gradient decent:
    how far your prediction was off from your actual / how far your change was

momentum:
    .9 is most common
    changes how the gradient is updated where it uses .1 * the derivative of this time and .9 * the last value
    basically an exponentially weighted moving average of the last few values

rmsprop:


adam:
    momentum + rmsprop
    keeps track of the exponential moving average of the steps
    and keeps track of the exponential moving average of the gradient

learning rate per batch:
    start small because the weights are random so we want to move slowly
        and if you jump around because of the big gradients related to the random weights they'll throw you off
    then when you get to sensable weights you want to have a higher learning rate where you end up moving in the right direction
    then as you get close the final answer you want to start slowing down to hone in
    when you have this low learning rate but you keep going in the right direction you might as well go faster aka high momentum
    and then if your jumping really far with your learning rate, don't compound it with high momentum, because it might throw you off
    one_cycle does this where it can let you do super convergence

fastai losses
    doesn't show you actual loss
    rather shows the exponentially weighted moving average of the loss so its easier to visualize

cross entropy loss:
    penalizes very confident wrong predictions highly
    requires activations coming into the loss function to add up to 1
        because they're all probabilities
    meaning you have to use a softmax non-linearity of your last layer

softmax:
    does e ** activations (becasue that'll make them all positive)
    then sums those numbers and divides them all by it
    sinlge lable multiclass classification

pytorch actually does softmax cross entropy loss
this means you might find your classifier outputs some crazy negs and pos numbers, it probably just needs a softmax at the end of it to use it

regularization in nueral nets:
    weight decay
    batchnorm
    dropout
    data augmentation

#-######################-#
#       lesson 6
#-######################-#

tabluar data:
    break everything into datetime parts

dropout:
    for each minibatch throw away a percentage of the activations
    common value of p (drop perc) is .5
    typically only in the hidden layers, not the input or output layers
    goal to make no one activation or set of activations memorize an input to output ultimately regularizing
    needs to be activated during trianing but NOT during inference aka prediction
    instead of doing this pytorch and other networks actually multiply all weights during training time by 1/p
    makes it so that you don't actually need to do anything during inference

batchnorm:
    bit of regularization
    bit of trining helper
    aids in reducing bumpyness of the loss vs epoch relationship
    ultimately lets you use higher learning rates
    works in minibatches
    its a layer that takes in activations and adds both multiplicative bias and added bias
    ultimately makes it easier to scale layers influence up or down by these two learnable parameters
    also its key that it uses momentum when determining the mean and variance it uses to calc bn
        takes an exponentially wieghted average of the mean and variance so they don't bounce around so much between mini batches
        but the mean and variance in jeremys words aren't the most important part of bn. so perhaps you could get away with just the biases
    smaller momentum leads to less bouncing of the mean and variance so it has less of a regularization affect

data augmentation:
    jeremy is most excited about it as a regularization method - mainly because there haven't eben definative studies on the matter
    nobody is doing data aug in text data or tabular or medical genomics anything like that
    decreases data req by like 5-10x because its basically free extra labeled data

convs:

average pool:

hook:

#-######################-#
#       lesson 7
#-######################-#

data blocks api:
    1. create an item list (il)
        list of items that it will create data from
        case of images its a list of images
        case of df its a list of samples
        il.items[0] gets you the first sample
    2. split data into train vs validation (sd)
        .split_by_folder .no_split for no validation
        cannot skip entirely
    3. label it and create a label list (ll)
        create the labels via methos like
        label_from_folder label_from_re
    4. setup transforms
        set transforms for both training and validation
        .transforms(([train transforms],[validation transforms]))
        these are applied when grabbing the data from disk (at least for images)
    5. create a data bunch
        set your batch size
        normalize data if your not using a pretrained model
        data.train_ds is a data set
        data.train_ds[0] will give an x and a y sample
        data.one_batch() will give a transformed data batch
        data.show_batch() will show samples and their labels

deep residual networks and learning (paper):
    they noticed that the deep (large param space) network performed worse than the shallonw one and said hmm
    figured he could skip all the conv layers at the end by putting a skip connection in and that the network would
        shouldn't perform any worse than the original smaller network
    skip connection is one where the input is fed around the layers so the network in theory the model can use
        set the weights to zero in those layers and bypass them altogether
        by adding them to the outputs (+)
    skip connections or res-blocks are revolutionary and pretty much can always outperform networks without (?)

fastai conv_layer:
    conv batchnorm relu chunks

visualizing the loss landscape (paper):
    they plotted loss over a x,y (two dimensions probably pca'ed down)
    then showed that res-blocks drastically smooth the loss landscape proving why res-blocks really work

resnets:
    genuinely really simple and easy to implement

densenets:
    same concept as res-blocks except instead of adding them to the output of a layer they're concatted
    concats each input to the next output essentially keeping the original input features the entire way
        through the layers
    very memory intensive but typically few params
    they work well with segmentation because they keep the input
    in practice theres something else going on that downsamples the dense concatted input since its a diff size than the input to the next layer

refactoring:
    do if often and consistenly, you'll make less mistakes when things are bundled

unet:
    imaging segmentation model that uses skip connections (concatting) across the down and upsampling parts
        pre res-net which is pretty clever
    downsampling part reduces pic down to is conv interpretation to somehting really dense like 512 x 7 x 7
    upsampling part increases each channel by conv'ing it against a larger version of the channel
        padding with pad pixels in between
        nearest neighbor interpolation

note:
    generally you want as much interactions between added or concatted inputs with the outputs of that channel as possible

gan to generate higher quality pics (decrappify):
    basic idea is to start with high qual pics, transform them into crappy versions of the same thing
    then train a model on the crappy input pic with the high quality output image as the 'truth' image

generative model:
    output is an image that created as opposed to a number of class

think about how loss function relates to your goal:
    *needs to mathmatically descibe what we want
    mse of pred to truth on pixel level gets the model to take care of really large issues in input image
        but doesn't solve the low to high res problem since the mse between pixels isn't a great metric
    before watching the rest im thinking that a loss function that includes the error of the min and max of small regions of the photo would be in the right direction

gan:
    actually does this by using a loss function that calls another model
    so the model you call is actually trying to classify the prediction and the truth as whether theyre the truth or the generated image
    so by using a critic t as the loss, and if we can fool it into thinking the generated image is teh truth, then its gotta be pretty good at generating images
    the critic is really just a binary cross entropy loss function
    you need to bounce between training the generator to create images and training the critic into being a good critiquer
    to make it work you need to stack losses of both the critic and the mse of pixels
        because otherwise you could be generating really good images that have no similarity to the original picture
    since each of those two losses are on different scales you need to get your pixel loss to be on the same order as the critic
        usually multiply by 50 to 200
    also since your updating the critic to get better and better as you go its hard to determine how well its working because the loss will be ablout the same over time
        so the best way to determine how well you're doing is by taking a look at the batches over the course of training

unets:
    use when the resolution of the input and output is roughly the same

reclaiming gpu mem:
    set something you know is using a lot of mem to = None
    then do gc.collect() to garbage collect

feature losses:
    grab some middle activations of the generator model and compare the pred and thruth activations of that layer and do mse on it
    using the idea that the features are just as important as the actual loss
    basically its trying to judge a prediction on the featuers which will represent the pic

generally when joining layers they're either added or concatted

jeremys advice:
    do something. 100% through
    when you get to that 80% finished point, add a readme make sure you have a proper install script
    finish things completely.

#-######################-#
#       Lesson 8 (2018)
#-######################-#

object detection:
    labeled dataset for bounding boxes
    steps
        1. classify largest 'thing' in image
        2. draw its bounding box
        3. classify whatever is in the bounding box (ideally teh original class)

coding tips:
    try to reduce vertical size of code
    try to reduce names to very short vars
    training_annotations == trn_anno

x,y coords:
    computer vision world thinks in width by height
    math/python/fastai/torch/numpy thinks in rows (downward) by cols (left to right)
    also converts bounding boxes to top left point, and bottom rigth point as opposed to what data is labeled as

vscode:
    jump to requires 'brew install ctags'
    shortcuts (https://code.visualstudio.com/shortcuts/keyboard-shortcuts-macos.pdf)

vscode navigation:
    navigate back/forward: ctrl + -, ctrl + shift + - (https://stackoverflow.com/questions/35424367/how-to-navigate-back-to-the-last-cursor-position-in-visual-studio-code)
    jump to def: cmd + click, highlight + f12
    find in workspace: cmd + t

cv2 (opencv) vs pil vs torchvision:
    open cv is fast because it unlocks pythons single thread

approach:
    a lot of work goes into setting up stuff to explore the dataset, functions etc
    need to get to a point with a new dataset that you can *rapidly explore*
    when starting with a new problem, start with easy wins and progessively update
    for example start with the largest item classifier
        1. get largest bounding box per image

    also, think of problem as inputs and outputs (or differnatial programming?)
    so to create a bounding box you need top x,y adn bottom x,y coords (which are continuous)
        so you need to get 4 activations from a neural net
    the other half is to figure out what loss function you want to use
        what makes the 4 numbers lower when theyre closer to the right numbers?
        mse works because it comparing each activation to the truth but overly penalizes bad errors
        l1loss (aka abs loss) works better? he recommends

creating functions:
    he writes psudocode of how he'd like to use the function
    then he writes the function to actually do it
    then run the original code
    also, hes all about ensuring what he writes is correct. not to the point of looking at all bounding boxes within image but judging it subjectively

lr find:
    plots loss per batch - so if you don't have many batches this won't work well
        might have to add n_skip=5 and n_skip_end=1
    normally skips the first 10 batches because theyre very low low and the last 5 because the loss shoots sky high

debugging:
    pdb.set_trace() - should look into this
    %debug (opens it up at the line of exception so you can inspect!)

fastai models:
    custom_head is a way to add a custom layer to a pretrained model
    where the 'head' is a nn.Sequential(Flatten(),nn.Linear(num_in,num_out))

#-######################-#
#       lesson 9 (2018)
#-######################-#

data aug and dependent vars:
    if any of the output is based on pixel values like bounding boxes and you use data aug to train
    you need to transform the dependent variable as well
    so in transforms you need to add the tfm_y=TfmType.Coord
    as usual need to check the data as you do it to double check your transforms aren't warping too much to create incorrect or overly wide bounding boxes

loss function:
    whatever makes this lower will optimize the network

rule of thumb:
    relu then batchnorm

dropout:
    also adding dropout even if you don't use it is a good idea because you can up the param and retain
    go with overfitting with a overly complex model then just up the p val
    **also loading a saved model with dropout unused makes it easier to add later on

custom dataset:
    requires playing with classes and all that
    but basic idea is to feed the model x as well as y1 (class) and y2 (bounding box)
    really just tying multiple ys to the same x. no big deal because the ys aren't used as inputs only as targets in loss or metrics

both bounding box and class:
    create activations in terms of adding the number of outs you need
    l1oss on first 4 activations
    crossent on 4: activations
    then make the loss function look at each appropriately

creating custom loss func:
    reqquires inputs and outputs
    also you can add multiple loss together
    make sure the multiple losses are around the same scale so one doesn't over power the other
        bounding box error in 100s
        cross entropy in 1s

arch for multi class (for arbitrary size 16):
    flattened end layer (yolo v1 + v2):
        4 x 16 bounding box
        num_classes x 16 for classifier
    or conv layer (yolo v3 + ssd use this. typically better performing):
        to convert the weights to a 4 x 4 x (4 + num_classes) tensor output
    why? because of receptive field of the activation of the convs
    he still flattens the last activaton layer because the numbers hvae been calculated via conv anyways and the loss function doesn't care

matching problem:
    loss function this time not only needs to classify and find bounding box
    but also needs to see if its close enough to that part of the image to make sense
    aka if you classify a dog in top right but the dog was really int he bottom left with bounding box there its really incorrect

my idea prior to watching:
    id say you take the center of the receptive field per conv strand and measure distance between the center of a bounding box
    so you have to shuffle a target across all strands and find when its distance is lowest
    then call that strand taken, and try all the each next target and do the same through the leftover strands?

actual solution:
    find overlap between target bounding box and receptive feild box (which is just the image split into 16)
    by taking the jaccard index aka area of overlap over total area of the recpetive feild + anything in the bounding box oustide of the recp field
    then you also take each recptive feild and find the overlap of the truth bounding box for each
    then take labels from 1st, and threshold labels for 2nd
    anchor box == receptive field

then loss function:
    then calculate loss only on the bounding box of the positive indexs in the matching
    also scale bounding boxes to be able to scale outside of the receptive feild

minor tweak:
    he doesn't use bceloss as the standard for multiclass because having a class of background is
        very difficult because its asking if the model knows the similarities between all backgrounds to predict them properly
    instead he makes it use bce and check if any of the classses exist, if no no no no no then its background
    this type of minor tweak usually makes a big difference. need to understand why

reality:
    this method is constained to just 16 anchor boxes
    need to make more anchor boxes to use the loss and better scale/shrink the guesses of the bounding boxes

focal loss:
    similar to bceloss except makes the loss variable when confidence in the prediction still isn't great
    basically tuning iffy guesses to not penalize the loss enough to have the loss function then work

reading papers:
    outlook on any method that involves multiple 'stages' or models to product an output
    typically these always get reduced to a single end to end model
    so he ignored obj det until ssd came up with a way to do it end to end

#-######################-#
#       lesson 10 (2018)
#-######################-#

#-######################-#
#       Bert
#-######################-#

link to bert: https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html

context-free vs contextual:
    word2vec and glove are context free - meaning they create the same embedding for the word
        bank whether it is used a 'bank fo a river' or 'bank heist'
    bert (and some other pre-trained representations) are contextual - meaning they generate
        a representaion for each word based on the other words in the sentence. while a uni-directional
        model would only see the previous words of the context, a bidirectional model (like bert)
        uses both words prior to and after the word of interest to generate its representation.

zero-shot:
    zero-shot learning is

attention:
    almost like a convnet type thing for language. where it layers multiple query-value filters
        on each word and then stacks them.

two pre-training methods similtaneously.
    these are unsupervised since they can be trained on raw text:
    1. mask a % of the words of a sentance, then try to predict them. and it can use the whole sentance to predict
    2. is this the next sentance?

then finetuning but instead of chopping off the end, or adding another layer, they use the same output layer and change the task for a given node


#-######################-#
#       MIT
#-######################-#

google automl:
    using reinforcement learning to tweak models and change optimization
    adanet:
        using reinforcement learning with ensemble with multiple architectures

data augmentatoin:
    taking the raw data and messing with it to give you much more context of it IRL
    automl does this - augments data in such a way that it optimizes for accuracy
    then you can take the optimal data aug method and apply it to other problems
        transfer learning in a very out of the box way (transfer the policies of augmenting)
    exciting because it will lead to learning alot from a little

synthetic data:
    genereate world first then train on actual data

multi-class vs multi-label:
    multi-class - single label per training example (can only be sunny or cloudy)
    multi-label - multiple labels per training exampel (can be sunny and cloudy)

brain vs dl:
    deep learning is synchronous
    brain learns asynchronously

backprop:
    he repeatedly talks about backprop being a poor way or the only way of doing things.
    geoff hinton says there should be a better way
    i'd really like to dig into evo learning and feature learning (?) where you hook and
        tie loss on mid points in a network
    take imagenet photos and first classify them as place, thing, animal type leanring
    i imagine this is what babies or anyone with a new topic does, they generalize everything, group everything

object detection:
    naive - create bounding boxes, then classify within them
    one shot - (yolo) based on features generate and classify in one go

semantic segmentation:
    classify at the pixel level to create overlays
    what would semantic segmentation be in language?
    at the word level, classify which are related to humor vs attention seeking vs deflection?
    i suppose you need a labeled dataset for semantic segmentation
    requires a image model (resnet encoder) + upsampling method (unet decoder)
    so.. would need a language model and some way to upsample back to the same 'space' as the input words
    would be useful in a business negotiation? to try to read somebody based on their actions/words phrasing?

embeddings:
    in practice to generate embeddings you want to train your model on a discriminative task
    by doing so the netowrk is better able to create a rich latent representation as opposed to something that might not suit the task

skip gram:
    2 words on either side
    i [am] eating a sandwich
    [am,i] [am,eating] [am,a
    input is word embedding, output is probability of being a neighborly word

sequence basd recommendations based on transactions as each input

lex friedman talks
https://www.youtube.com/watch?v=53YvP6gdD7U

#-######################-#
#       Jerry Howard Lex Freidman Talk
#-######################-#

# url = https://www.youtube.com/watch?v=J6XcP4JOHmk

Mentions:
    Dawn Bench
    Space repitition  - anki, supermemo

Active learning:
    Platform ai - does active learning
    labeling in the loop with humans

Data Creation:
    Always important to figure out what and how data is created


#-######################-#
#       Deploying ML
#-######################-#

Service level agreements:
    max response time
    max retrain time
    availability
    quality/confidence
    monitor

Store lineage of models overtime
    metadata like version, name, type, ml framework, eval metric
    serialized object
    host via api

When hosting need to think about:
    how to configure the app
    how many instances/ram
    connection to db/credentials
    creashes and recovery
    load balancing and workload scaling

Continuous Integration
    Gitlab CI/CD


Services
    Cloud Foundry

#-######################-#
#       Recommendations
#-######################-#

# url - https://developers.google.com/machine-learning/recommendation/overview/terminology

items - entities recommended
query - information used to make recommendations
    user features - user id, user hx
    item features - item id item metadata
    context - time of day, user's device
embeddings
    vector representations of discrete items

typical steps:
    candidate generation - reduce total items down to a manageable set 100s of thousands
    scoring - reduce the candidates to a set of
    re-ranking - is like applying the business rules boosting or negating based on user pref and fresh content

candidate generation:
    content based:
        similarities between the items themselves.
    collaborative based:
        similarities between queries and items simultanteously

embedding space:
    items and queries (potentially users and context) are mapped  to embedding vectors
    captures some latent structure of the item or query set
    vectors can be evaluated by "closeness" with a similarity measure

similarity measure:
    produces a scalar to represent to vectors "closeness" or distance
    the closer they are the more similar, however different measures will produce different results
    cosine distance:
        angle between the vectors
        basically omits vector magnitudes (ie making them not matter)
        used frequently in text due to this fact - https://cmry.github.io/notes/euclidean-v-cosine
    dot product:
        cosine angle multiplied by the product of norms
    euclidean:
        standard root squared

content based:
    pros:
        can easily scale to large number of users
        can recommend niche items that few others are interested in
    cons:
        hand-engineered features requires domain knowledge
        can only make recs on existing interests, cannot expand interests

collaborative:
    explicit - requires questionaires to be filled out
    implicit - generated from user behavior
    learns embeddings for each item and user in the same space automatically

    matrix factorization:
        embeddings are learned such that the user_emb*item_emb.T ~= feedback_matrix
        due to the sinze of user/item_embs being small this can be significatntly more compact than learning the full matrix
        objective function:
            sum squared errors - bad because it doesn't assume 0s for empties
            frobieus error - mse with 0s for unwatched
            weighted matrix factorization - weighted mixture of sum over observed and unobserved entries and function of frequency

    pros:
        no domain knowledge required
        model can help users find new interests (look a like)
        doesn't need context features
    cons:
        cannot handle new items/features (cold start problem) but can be addressed by average embeddings from same category or user
        cannot easily incorporate side information

softmax (nn):


#-######################-#
#       Algorithm Analysis
#-######################-#

readability
    very important but unrelated to actual algorithm 'betterness'

efficiency
    space requirements
    execution time

due to differences in programming language, hardware, underlying software, etc.
    it is very difficult to compare the actual execution times between different
    implementations of the same algorithm. thats why big-o was created

big-o
    requires you to think of solving an algo in terms of 'basic unit of computation'
    for example the number of assignment statements like 'x = 0'
    the order of magnitude function (Big-O) describes the part of T(n) that increases the fastest
        as the value of n increases (the size of the problem increases)

for a looping summation, the time it takes to solve any problem of size n is:
    T(n) = 1 + n <- is same as -> O(n)
    T(n) = 5n^2 + 27n + 1005 <- is same as -> O(n^2)

sometimes performace depends on the values of the data rather than the size of the problem
    in these cases we charcterze them in:
        best
        worst
        average

common big-o functions
    1 - constant
        single assignments/calculations
    log(n) - logarithmic
    n - linear
        for loop assignments/calcs
    n log(n) - log linear
        sorting
        if the problem size is cut in half each iteration
    n^2 - quadratic
        double for loop assignments/calcs
        sorting
    n^3 - cubic
    2^n - exponential
    n! - factorial
        brute force for anagrams

many times algorithms will sacrifice space to save time
    i.e. storing lists or a running minimum will reduce run time

#-######################-#
#       Data Structures Performance
#-######################-#

lists
    O(1) operations
        indexing
        assigning val to index
        appending
    O(k)
        concating (k=size of list being concated)
    O(n)
        pop(0) because the entire list needs to shift over!
        contains (in) can you believe it?!

list creation
    list(range()) - best
    [i for i in range()] - twice as good as loop below
    for i in range(): append(i)
    for i in range(): l + [i] - worst by a long shot


dicts
    O(1) operations
        get item
        set item
        contains (in)
    O()

**need to learn how to use timeit.Timer() to measure o(n)

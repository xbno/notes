fastai course v3 notes

# found useful
free -m
docker exec -it container_id bash
jupyter notebook list
# sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/data:/data -v /home/xbno/ml/course-v3:/course-v3 --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
sudo docker run --runtime=nvidia -d -p 8888:8888 -v /home/xbno/course-v3/data:/data -v /home/xbno/course-v3/code/course-v3-me:/notebooks/course-v3-me -v /home/xbno/course-v3/pretrained/.torch:/root/.torch --shm-size 26G paperspace/fastai:1.0-CUDA9.2-base-3.0-v1.0.6
find / -name "stage-1-50*"
docker container stop $(docker container ls -aq)
docker container rm $(docker container ls -aq)

# functions
doc()
    has links to documentation
help()
    shows info

ImageDataBunch()
    main data object that pulls in data from directory etc
.from_df()
.from_csv()
.from_folder()
    most common way to load images, imagenet style
    /train/categories and /test/categories

learn()
    main object in fastai that encompases both data and a model
.load()
    load saved model, dunno where
.lr_find()
    saves the recorder below based on mock loss of the data and lr
.recorder.plot()
    display lr curve


# notes
high loss predictions
    ones that our model is most confident in but got wrong

training progression
    start with high lr (~.001 aka 1e-3) or (3e-3 is a jeremys fav) fine tuning last layers
    follow up with a full tune of all layers with a lr_max of something between 1e-6
    and soemthign 10x smaller than what you started with or clearly not close to the
    edge of lr cliff

    you know you have a good training rate if your error doesn't explode and it
    isn't sluggishly slow either.

    time to get more data when you select a proper learning rate, and your loss goes
    down and then begins going up, and you're still unhappy with your accuracy

validation loss:
    if its enormous, the learning rate is too high

train loss:
    always aim to train loss lower than validation loss. if its higher your underfitting.
    bump up # of epochs or learning rate.
    train the last bit at a lower learning rate?
    or reduce regularization by adding weight ecay, data aug or
    ***this isn't stated by all in deep learning, so may read otherwise***

overfitting in DL:
    your error rate improves for a while and then starts getting worse

oversampling shouldn't be required unless you find your lower samples

if run out of mem, change the bs (batchsize) in the databunch object

recommended to seed prior to running the image data bunch so it picks a repeatable set
    of images because then you can concretely confirm if hyperparam tuning actually helps
    the model since it will be run on the same images again: np.random.seed(42)

if you find activation maps within a convnet that produces zeros for a lot of different
    inputs, you probably have deadfilters. this is a symptom of high learning rates

# lesson 2

tensor
    rank 1 tensor: torch.tensor(np.array([1,2,3]))
    rank 3 tensor: image rows x columns x channels (r,g,b)
    rank 4 tensor: num_images x rows x columns x channels (r,g,b)

gradient decent:
    calculates the loss on the whole data set (x) each time it updates params
    ex:
        y_hat = x@a
        loss = mse(y, y_hat)

stocastic gradient decent:
    calculates the loss on a sample of the dataset (x[rand_idx]) and updates params
    will bounce around even without a high learning rate
    ex:
        rand_idx = np.random.choice([i for i in range(n)],s,replace=False)
        y_hat = x[rand_idx]@a
        loss = mse(y[rand_idx], y_hat)


sgd vs gd:
    the max learning rate appropriate will change based on the batch size etc.
    meaning choosing whats in your batches is REALLY important (maybe this is already done?)
    which is why lightfm's warp was sneaky good, because it chose only to do updates on samples
        that were very wrong. look into this

learning rate (lr):
    is what we mulitply our gradient by to update our params

epoch:
    one complete run through all our data. when doing batches, this means all batches
        seen have included all data
    generally want to keep epochs down as much as possible, since the more epochs we see
        the more times we've seen the same data which gets makes it more likely to overfit

architecture:
    the mathmatical model you're fitting the parameters to
    y = x1*a1+x2*a2 (a is the params), which is m*x + b
    y = resnet34(image)

parameters (weights or coeficients):
    are the numbers that you're updating

# lesson 3

pytorch dataset/fastai databunch logical structure is:
    torch.utils.DataSet() is used by torch.utils.data.DataLoader() is used by fastai.
    DataLoader:
        combines dataset into a minibatch ready for model consumption
    DataBunch:
        composed of two DataLoaders for train and validation sets

fastai DataBlock api:
    creates everything fort he databunch object

tfms get_transforms():
    filp_vert = True

accuracy:
    single-class:
        with a standard single class problem there will be n outputs of the model
        each will corespond to the probability (because the last layer is a sigmoid)
        of the class. therefore you use an argmax to find the most likely class based
        on the class with the highest probability
        func: accuracy
    multi-class:
        with a multi class problem there will be numerous outputs each that an image
        or input could be labeled with. therefore you need to select a threshold
        an output neuron needs to exceed to be labeled.
        func: accuracy_threshold(thresh=.2)

partial:
    **since we always want to call this function with a thesh=.2 you can use
    a python 3 thing called acc_02 = partial(func,keyword=value) to call it.**

    def average(l,num):
        return np.random.choice(l,num,replace=False).mean()

    def a3(l):
        return average(l,3)

    a3 = partial(average,num=3)

    average([1,2,3],3) == a3(l)

for misclassified data:
    recommends taking the saved original model, using a new databunch with the
    misclassified data, and finetuning (unfreezing?) model. test and play most likely
    but perhaps using a higher lr

lr_find:
    we want to hit the steepest negative slope so that we can slide down to the bottom.
    typically before you unfreeze sliding from higher 1e-4ish to the bottom ~1e-2ish

progressive resizing:
    the idea of training the model on 64x64 then 128x128 then 256x256.
    a form of transfer learning?
    how does the same model take a different input so easily?
    **seems as though it is less sensitive to frozen training as you progressively
    train on larger images. perhaps lower number of fits or unfreeze quicker

fit_one_cycle:
    actually bumps the lr up at the beginning and then brings it down

    uses learning rate annealing (gradually decreasing the lr while fitting)

plot_losses:
    if you find the loss increases a bit and then decreases you've found a good lr

mixed precision training:
    16bits instead of 32bits precision
    learner.to_fp16()

normalizing:
    when training with a pretrained model, you don't want to normalize your data
    prior to inputting since you'll normolize the data to itself and lose the
    learning the pretrained model has built. instead

    # lesson 4

    language model:
        a model that learns to predict the next word of the sequence, nearly useless on its own

    typical approach:
        1. take a full wiki language model
        2. finetune the model to the specific language corpus (imdb reviews)
        3. use the language model to create a classifier which will output the answers.
            requires the same vocab as the finetuned lnaguage model (#2)

    typical acc with a lanuague model:
        for specific domain like legal or medical acc can be >= .5
        for imdb reviews acc ~.3 is pretty good

    numericalization:
        basically tokenize words
        do

    language models (rnns in general?):
        encoder - the bit that learns how to undertsand the sentance so far (context?)
        generater - the bit that learns how to predict the next word

    tabular data in deeplearning:
        lessens the requirement of making
